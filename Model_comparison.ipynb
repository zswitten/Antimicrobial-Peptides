{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobwitten/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "from copy import deepcopy\n",
    "%matplotlib inline\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.manifold import TSNE\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from Bio import pairwise2\n",
    "from Bio.SubsMat import MatrixInfo as matlist\n",
    "import seaborn as sns\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, LSTM, Conv2D, Conv1D, MaxPooling1D, MaxPooling2D, Flatten, ZeroPadding1D, SimpleRNN, Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "from Bio import pairwise2\n",
    "from Bio import SeqIO\n",
    "from Bio.SubsMat import MatrixInfo as matlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CHARACTER_LIST = [u'A', u'C', u'E', u'D', u'G', u'F', u'I', u'H', u'K', u'M', u'L', u'N', u'Q', u'P', u'S', u'R', u'T', u'W', u'V', u'Y']\n",
    "nchar = len(CHARACTER_LIST)\n",
    "CHARACTER_LIST.sort()\n",
    "CHARACTER_DICT = set([u'A', u'C', u'E', u'D', u'G', u'F', u'I', u'H', u'K', u'M', u'L', u'N', u'Q', u'P', u'S', u'R', u'T', u'W', u'V', u'Y'])\n",
    "MAX_SEQUENCE_LENGTH=46\n",
    "MAX_MIC = 4\n",
    "FONT_TO_USE = 'Arial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_embedding(matrix_dict,min_corr=0.99):\n",
    "    distance_matrix = np.zeros([len(CHARACTER_LIST),len(CHARACTER_LIST)])\n",
    "    for i,char1 in enumerate(CHARACTER_LIST):\n",
    "        for j,char2 in enumerate(CHARACTER_LIST):\n",
    "            if (char1,char2) in matrix_dict.keys():\n",
    "                distance_matrix[i,j] = (matrix_dict[(char1,char1)]+matrix_dict[(char2,char2)]-2*matrix_dict[(char1,char2)]+0.)\n",
    "            else:\n",
    "                distance_matrix[i,j] = (matrix_dict[(char1,char1)]+matrix_dict[(char2,char2)]-2*matrix_dict[(char2,char1)]+0.)\n",
    "    G_matrix = np.zeros([nchar,nchar])\n",
    "    for i in range(nchar):\n",
    "        for j in range(nchar):\n",
    "            G_matrix[i,i] += distance_matrix[i,j]**2/nchar\n",
    "            for k in range(nchar):\n",
    "                G_matrix[i,i] -= distance_matrix[j,k]**2/(2*(nchar**2))\n",
    "\n",
    "    for i in range(nchar):\n",
    "        for j in range(nchar):\n",
    "            G_matrix[i,j] = (G_matrix[i,i]+G_matrix[j,j]-distance_matrix[i,j]**2)/2\n",
    "    values,vectors = np.linalg.eigh(G_matrix)\n",
    "    corr = 0\n",
    "    n_dimensions = 0\n",
    "    while corr<min_corr:\n",
    "        n_dimensions += 1\n",
    "        sqrt_lambda_matrix = np.zeros([n_dimensions,n_dimensions])\n",
    "        for i in range(n_dimensions):\n",
    "            sqrt_lambda_matrix[i,i] = np.sqrt(values[i-n_dimensions])\n",
    "        u_matrix = vectors[:,nchar-n_dimensions:nchar]\n",
    "        product = np.matmul(sqrt_lambda_matrix,np.transpose(u_matrix))\n",
    "        embedding_matrix = np.zeros([n_dimensions,nchar])\n",
    "        for i in range(nchar):\n",
    "            for j in range(n_dimensions):\n",
    "                embedding_matrix[j,i] = product[n_dimensions-1-j,i]\n",
    "        embedding_matrix = np.transpose(embedding_matrix)\n",
    "        reconst_dist_matrix = np.zeros([nchar,nchar])\n",
    "        for i in range(nchar):\n",
    "            for j in range(nchar):\n",
    "                to_set = np.linalg.norm(embedding_matrix[i,:]-embedding_matrix[j,:])\n",
    "                reconst_dist_matrix[i,j] = to_set\n",
    "        old=[]\n",
    "        reconst=[]\n",
    "        for i in range(nchar-1):\n",
    "            for j in range(i+1,nchar):\n",
    "                old.append(distance_matrix[i,j])\n",
    "                reconst.append(reconst_dist_matrix[i,j])\n",
    "        corr = np.corrcoef(old,reconst)[0,1]\n",
    "    embedding_dict = {}\n",
    "    for i,char in enumerate(CHARACTER_LIST):\n",
    "        embedding_dict[char] = embedding_matrix[i].tolist()\n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each amino acid its own group\n",
    "character_to_index = {\n",
    "    (character): i\n",
    "    for i, character in enumerate(CHARACTER_DICT)\n",
    "}\n",
    "\n",
    "index2character = {\n",
    "    value: key\n",
    "    for key, value in character_to_index.items()\n",
    "}\n",
    "\n",
    "# Generate the input vectors for our model\n",
    "# Each vector is two dimensional\n",
    "# The first dimension represents the number of characters in the sequence (46 characters)\n",
    "# Each character is a vector of length equal to the number of groupings of amino acids\n",
    "# This grouping can be 1-1 (each amino acid gets its own group), or coarser\n",
    "def df_to_input_vec(df,shuffle = False,embed_dict=None):\n",
    "    cterminal_amidation = np.array(df.has_cterminal_amidation)\n",
    "\n",
    "    vectors = []\n",
    "    for row in df.iterrows():\n",
    "        vectors.append(row_to_vector(row[1],embed_dict=embed_dict, shuffle_sequence=shuffle))\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "    labels = np.array(df.value)\n",
    "    sample_weights = np.full(len(labels), 1)\n",
    "    return vectors, labels, sample_weights\n",
    "\n",
    "def generate_random_sequence(alphabet, length_of_sequence_min=0, length_of_sequence_max=MAX_SEQUENCE_LENGTH, include_C = True):\n",
    "        sequence = ''\n",
    "        choices = [char for char in alphabet if (include_C or char !='C')]\n",
    "        counter = 0\n",
    "        length_to_use = -10\n",
    "        while counter<20 and (length_to_use < length_of_sequence_min or length_to_use > length_of_sequence_max):\n",
    "            length_to_use = random.choice(SEQ_LENGTHS)\n",
    "            counter += 1\n",
    "        for _ in range(length_to_use):\n",
    "            sequence += random.choice(choices)\n",
    "        has_cterminal_amidation = random.uniform(0, 1)\n",
    "\n",
    "        return {\n",
    "            'sequence': sequence,\n",
    "            'has_cterminal_amidation': has_cterminal_amidation>0.5\n",
    "#             Used to be 0.5 but only 34% of positive data has amidation OOPS\n",
    "        }\n",
    "\n",
    "def add_random_negative_examples(vectors, labels, sample_weights, ratio, max_mic = None, include_cysteine = True):\n",
    "    if not max_mic:\n",
    "        max_mic = max(labels)\n",
    "    # We will add randomly chosen sequences as negative examples\n",
    "    # We will double the length of our training set\n",
    "\n",
    "    len_vectors = ratio * len(vectors)\n",
    "    negative_rows = []\n",
    "    for i in range(len_vectors):\n",
    "        negative_rows.append(row_to_vector(generate_random_sequence(list(CHARACTER_DICT),include_C = include_cysteine)))\n",
    "    negative_vectors = np.array(negative_rows)\n",
    "    vectors = np.concatenate((vectors, negative_vectors))\n",
    "    negative_labels = np.full(len_vectors, max_mic)\n",
    "    labels = np.concatenate((labels, negative_labels))\n",
    "    # Weight all samples equally\n",
    "    sample_weights = np.concatenate((sample_weights, np.full(len_vectors, 1)))\n",
    "    return vectors, labels, sample_weights\n",
    "\n",
    "def generate_train_test_splits(\n",
    "        vectors, labels,\n",
    "        extra_training_vectors=[], extra_training_labels=[], extra_sample_weights=[],\n",
    "        cutoff=0.85\n",
    "):\n",
    "    cutoff = int(cutoff * len(labels))\n",
    "    idx = range(len(vectors))\n",
    "    random.shuffle(idx)\n",
    "    reordered_vectors = vectors[idx]\n",
    "    reordered_labels = labels[idx]\n",
    "    reordered_sample_weights = sample_weights[idx]\n",
    "    if len(extra_training_vectors) > 0:\n",
    "        train_x = np.concatenate((reordered_vectors[:cutoff], extra_training_vectors))\n",
    "        train_y = np.concatenate((reordered_labels[:cutoff], extra_training_labels))\n",
    "        train_sample_weights = np.concatenate((reordered_sample_weights[:cutoff], pa_sample_weights))\n",
    "    else:\n",
    "        train_x = reordered_vectors[:cutoff]\n",
    "        train_y = reordered_labels[:cutoff]\n",
    "        train_sample_weights = reordered_sample_weights[:cutoff]\n",
    "    test_x = reordered_vectors[cutoff:]\n",
    "    test_y = reordered_labels[cutoff:]\n",
    "    return train_x, train_y, test_x, test_y, train_sample_weights\n",
    "\n",
    "# Convolutional NN\n",
    "def conv_model(embed_length = (len(character_to_index)+1),kernelsize = 5):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(ZeroPadding1D(\n",
    "        kernelsize, input_shape = (MAX_SEQUENCE_LENGTH, embed_length)\n",
    "    ))\n",
    "    model.add(Conv1D(\n",
    "        64,\n",
    "        kernel_size = kernelsize,\n",
    "        strides = 1,\n",
    "        activation = 'relu',\n",
    "        #input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(64, kernelsize, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Convolutional NN\n",
    "def conv_model_set_dropout(dropout_level=0.5,embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(ZeroPadding1D(\n",
    "        5, input_shape = (MAX_SEQUENCE_LENGTH, embed_length)\n",
    "    ))\n",
    "    model.add(Conv1D(\n",
    "        64,\n",
    "        kernel_size = 5,\n",
    "        strides = 1,\n",
    "        activation = 'relu',\n",
    "        #input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(dropout_level))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Convolutional NN\n",
    "def conv_model_move_dropout(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(ZeroPadding1D(\n",
    "        5, input_shape = (MAX_SEQUENCE_LENGTH, embed_length)\n",
    "    ))\n",
    "    model.add(Conv1D(\n",
    "        64,\n",
    "        kernel_size = 5,\n",
    "        strides = 1,\n",
    "        activation = 'relu',\n",
    "        #input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Convolutional NN\n",
    "def one_layer_conv_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(ZeroPadding1D(\n",
    "        5, input_shape = (MAX_SEQUENCE_LENGTH, embed_length)\n",
    "    ))\n",
    "    model.add(Conv1D(\n",
    "        64,\n",
    "        kernel_size = 5,\n",
    "        strides = 1,\n",
    "        activation = 'relu',\n",
    "        #input_shape = (MAX_SEQUENCE_LENGTH, len(character_to_index) + 1)\n",
    "    ))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def lstm_model(embed_length = (len(character_to_index)+1),learn_rate=None):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(LSTM(\n",
    "        64,\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, embed_length),\n",
    "    ))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "#     model.add(Dense(100, kernel_initializer='normal'))\n",
    "# #     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(20, kernel_initializer='normal'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "#     model.add(Dense(1, kernel_initializer='normal'))\n",
    "    if learn_rate is None:\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='mean_squared_error',optimizer=Adam(lr=learn_rate))\n",
    "    return model\n",
    "\n",
    "def two_layer_recurrent_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(SimpleRNN(\n",
    "        64,\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, embed_length),return_sequences=True\n",
    "    ))\n",
    "    model.add(SimpleRNN(64))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "#     model.add(Dense(100, kernel_initializer='normal'))\n",
    "#     model.add(Dense(20, kernel_initializer='normal'))\n",
    "#     model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def three_layer_recurrent_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(SimpleRNN(\n",
    "        64,\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, embed_length),return_sequences=True\n",
    "    ))\n",
    "    model.add(SimpleRNN(64,return_sequences=True))\n",
    "    model.add(SimpleRNN(64))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "#     model.add(Dense(100, kernel_initializer='normal'))\n",
    "#     model.add(Dense(20, kernel_initializer='normal'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(20, activation='relu'))\n",
    "#     model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def one_layer_recurrent_model(embed_length = (len(character_to_index)+1), learn_rate = None):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(SimpleRNN(\n",
    "        64,\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, embed_length),return_sequences=False\n",
    "    ))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "#     model.add(Dense(100, kernel_initializer='normal'))\n",
    "#     model.add(Dense(20, kernel_initializer='normal'))\n",
    "#     model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    if learn_rate is None:\n",
    "        model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=learn_rate))\n",
    "    return model\n",
    "\n",
    "def bilstm_model(embed_length = (len(character_to_index)+1)):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        64\n",
    "    ),input_shape=(MAX_SEQUENCE_LENGTH, embed_length)))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "#     model.add(Dense(100, kernel_initializer='normal'))\n",
    "# #     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(20, kernel_initializer='normal'))\n",
    "#     model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def birecurrent_model(embed_length = (len(character_to_index)+1),learn_rate=None):\n",
    "    model = keras.models.Sequential()\n",
    "#     model = keras.models.Sequential()\n",
    "    model.add(Bidirectional(SimpleRNN(\n",
    "        64,return_sequences=False\n",
    "    ),input_shape=(MAX_SEQUENCE_LENGTH, embed_length)))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "#     model.add(Dense(100, kernel_initializer='normal'))\n",
    "#     model.add(Dense(20, kernel_initializer='normal'))\n",
    "#     model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    if learn_rate is None:\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='mean_squared_error',optimizer=Adam(lr=learn_rate))\n",
    "    return model\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self,models,predict_method,max_mic_buffer=0.1):\n",
    "        self.models = models\n",
    "        self.predict_method = predict_method\n",
    "        self.max_mic_buffer = max_mic_buffer\n",
    "        \n",
    "    def combine_predictions(self,predictions):\n",
    "        if self.predict_method is 'average':\n",
    "            return np.mean(predictions)\n",
    "        elif self.predict_method is 'classify_then_average':\n",
    "            actual_predictions = []\n",
    "            for prediction in predictions:\n",
    "                if prediction < MAX_MIC - self.max_mic_buffer:\n",
    "                    actual_predictions.append(prediction)\n",
    "            if float(len(actual_predictions))/float(len(predictions))>=0.49:\n",
    "                return np.mean(predictions)\n",
    "            else:\n",
    "                return MAX_MIC\n",
    "        else:\n",
    "            print 'predict_method not recognized'\n",
    "            return -100\n",
    "        \n",
    "    def predict(self,test_x):\n",
    "        all_predictions = []\n",
    "        combined_predictions = []\n",
    "        for model in self.models:\n",
    "            all_predictions.append(model.predict(test_x))\n",
    "        for i in range(len(test_x)):\n",
    "            combined_predictions.append(self.combine_predictions([all_predictions[k][i] for k in range(len(self.models))]))\n",
    "        return combined_predictions\n",
    "    \n",
    "    def evaluate(self,test_x,test_y):\n",
    "        predictions = self.predict(test_x)\n",
    "        correctly_classified_error = np.mean([(actual - predicted) ** 2 for actual, predicted in zip(test_y, predictions) if actual < MAX_MIC and predicted < MAX_MIC - self.max_mic_buffer])    \n",
    "        all_error = np.mean([(actual - predicted) ** 2 for actual, predicted in zip(test_y, predictions)])    \n",
    "        all_active_error = np.mean([(actual - predicted) ** 2 for actual, predicted in zip(test_y, predictions) if actual < MAX_MIC])    \n",
    "        return correctly_classified_error,all_active_error, all_error\n",
    "    \n",
    "    def evaluate_as_classifier(self,test_x,test_y):\n",
    "        true_positives=0\n",
    "        true_negatives=0\n",
    "        false_positives=0\n",
    "        false_negatives=0\n",
    "        all_predicted=self.predict(test_x)\n",
    "        for i in range(len(test_y)):\n",
    "            actual=test_y[i]\n",
    "            predicted=all_predicted[i]\n",
    "            if actual<MAX_MIC-0.0001:\n",
    "                if predicted<MAX_MIC - self.max_mic_buffer:\n",
    "                    true_positives+=1\n",
    "                else:\n",
    "                    false_negatives+=1\n",
    "            else:\n",
    "                if predicted<MAX_MIC - self.max_mic_buffer:\n",
    "                    false_positives += 1\n",
    "        #             print vector_to_amp(test_x[i])\n",
    "        #             print 'predicted: '+repr(predicted)+', actual: '+repr(actual)\n",
    "#                     print '>p'+repr(false_positives)+'_'+repr(predicted)\n",
    "#                     print vector_to_amp(test_x[i])['sequence'].replace('_','')\n",
    "                else:\n",
    "                    true_negatives += 1\n",
    "        return true_positives,true_negatives,false_positives,false_negatives\n",
    "        \n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Enter an element of a result dictionary into df-ready row\n",
    "# Standardize units of MIC\n",
    "def standardize_to_uM(concentration, unit, sequence):\n",
    "    concentration = concentration.replace(' ', '')\n",
    "    try:\n",
    "        concentration = float(concentration)\n",
    "    except:\n",
    "        return None\n",
    "    if unit == 'uM' or unit == u'\\xb5M' or unit == u'uM)':\n",
    "        return concentration\n",
    "    elif unit == 'ug/ml' or unit == u'\\xb5g/ml' or unit == u'ug/ml)':\n",
    "        try:\n",
    "            molWt = ProteinAnalysis(sequence).molecular_weight()\n",
    "        except ValueError:\n",
    "            return None\n",
    "        return concentration * 1000/molWt\n",
    "    elif unit == 'nmol/g' or unit == 'pmol/mg':\n",
    "        #1g, at density of 1g/mL, is 1mL, so nmol/g is nmol/mL = umol/L = uM yay!\n",
    "        return concentration\n",
    "    else:\n",
    "        # print 'Unit not recognized: ' + unit\n",
    "        return None\n",
    "    \n",
    "def convert_result_to_rows(sequence, result):\n",
    "    rows = []\n",
    "    if 'bacteria' not in result:\n",
    "        return rows\n",
    "    for bacterium, strain in result['bacteria']:\n",
    "        \n",
    "        rows.append({\n",
    "            'bacterium': bacterium,\n",
    "            'strain': strain,\n",
    "            'sequence': sequence.upper(),\n",
    "            'url_source': result['url_sources'][0],\n",
    "            'value': standardize_to_uM(\n",
    "                result['bacteria'][(bacterium, strain)]['value'],\n",
    "                result['bacteria'][(bacterium, strain)]['unit'],\n",
    "                sequence\n",
    "            ),\n",
    "            'modifications': result['modifications'] if 'modifications' in result else [],\n",
    "            'unit': 'uM'\n",
    "        })\n",
    "        if rows[-1]['value']:\n",
    "            rows[-1]['value'] = np.log10(rows[-1]['value'])\n",
    "    return rows\n",
    "\n",
    "# Remove sequences with amino acids that aren't well-defined\n",
    "def strip_sequences_with_char(df, bad_char):\n",
    "    return df[~df.sequence.str.contains(bad_char)]\n",
    "\n",
    "# We'll want to strip off any sequences with modifications that could be hard to replicate\n",
    "# Their effects are too complex for the model\n",
    "def is_modified(modifications_list):\n",
    "    return len(modifications_list) > 0\n",
    "\n",
    "# However, C-Terminal Amidation is common enough that we make an exception\n",
    "CTERM_AMIDATION_TERMS = ['C-Terminal amidation','C-Terminus: AMD','C-Terminal','C-termianal amidation']\n",
    "\n",
    "def has_non_cterminal_modification(modifications_list):\n",
    "    return any(['C-Term' not in modification for modification in modifications_list])\n",
    "\n",
    "def has_unusual_modification(modifications_list):\n",
    "    return any([is_uncommon_modification(mod) for mod in modifications_list])\n",
    "\n",
    "def has_cterminal_amidation(modifications_list):\n",
    "    return any([is_cterminal_amidation(mod) for mod in modifications_list])\n",
    "\n",
    "def has_disulfide_bonds(modifications_list):\n",
    "    return any([is_disulfide_bond(mod) for mod in modifications_list])\n",
    "\n",
    "def is_cterminal_amidation(mod):\n",
    "    for term in CTERM_AMIDATION_TERMS:\n",
    "        if term in mod:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_disulfide_bond(mod):\n",
    "    return 'disulfide' in mod.lower()\n",
    "\n",
    "def is_uncommon_modification(mod):\n",
    "    return (not is_cterminal_amidation(mod)) and (not is_disulfide_bond(mod))\n",
    "\n",
    "def datasource_has_modifications(cell):\n",
    "    # Everything except CAMP and YADAMP has modification data\n",
    "    return not any([s in cell for s in no_modification_data_sources])\n",
    "\n",
    "def sequence_has_modification_data(cell):\n",
    "    # If the sequence is labeled modifictationless in another database it's OK\n",
    "    return cell in sequences_containing_modifications\n",
    "\n",
    "def sequence_to_vector(sequence, cterminal_amidation,embed_dict=None):\n",
    "# It looks like this truncates any sequence after max_sequence_length (which is length of 95th percentile longest peptide)\n",
    "# I just add cterminal amidation as the amino acid after the last real amino acid (if the amino acid gets truncated\n",
    "# then the cterminal amidation also gets cut off)\n",
    "    if embed_dict==None:\n",
    "        default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "            default[i][character_to_index[character]] = 1\n",
    "        if len(sequence)<MAX_SEQUENCE_LENGTH:\n",
    "            default[len(sequence)][-1]=cterminal_amidation\n",
    "    else:\n",
    "        default = np.zeros([MAX_SEQUENCE_LENGTH,len(embed_dict['A'])+1])\n",
    "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "            embedding = deepcopy(embed_dict[character])\n",
    "            embedding.append(0)\n",
    "            for k,val in enumerate(embedding):\n",
    "                default[i,k] = val\n",
    "            if len(sequence)<MAX_SEQUENCE_LENGTH:\n",
    "                for i in range(len(sequence),MAX_SEQUENCE_LENGTH):\n",
    "                    default[i,-1]=1\n",
    "    return default\n",
    "\n",
    "def find_character(character2index, character):\n",
    "    for key in character2index:\n",
    "        if character in key:\n",
    "            return character2index[key]\n",
    "    return -2\n",
    "\n",
    "def row_to_vector(row, embed_dict=None,shuffle_sequence=False):\n",
    "    sequence = list(row['sequence'])\n",
    "    if shuffle_sequence:\n",
    "        random.shuffle(sequence)\n",
    "    cterminal_amidation = row['has_cterminal_amidation']\n",
    "    return sequence_to_vector(sequence,cterminal_amidation,embed_dict=embed_dict)\n",
    "\n",
    "def old_row_to_vector(row, shuffle_sequence=False):\n",
    "    sequence = list(row['sequence'])\n",
    "    if shuffle_sequence:\n",
    "        random.shuffle(sequence)\n",
    "    cterminal_amidation = row['has_cterminal_amidation']\n",
    "    default = np.zeros([MAX_SEQUENCE_LENGTH, len(character_to_index) + 1])\n",
    "\n",
    "    for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
    "        default[i][find_character(character_to_index, character)] = 1\n",
    "        default[i][-1] = cterminal_amidation\n",
    "\n",
    "    return default\n",
    "\n",
    "def vector_to_amp(vector):\n",
    "    sequence = ''\n",
    "    has_cterm = False\n",
    "    for v in vector:\n",
    "        nonzeros = np.argwhere(v[:len(character_to_index)])\n",
    "        if len(nonzeros) > 1:\n",
    "            print(\"?????\")\n",
    "        elif len(nonzeros) == 0:\n",
    "            sequence += '_'\n",
    "        else:\n",
    "            sequence += index2character[np.argwhere(v)[0][0]]  # First one\n",
    "        if v[-1]>0:\n",
    "            has_cterm=True\n",
    "    return {\n",
    "        'sequence': sequence,\n",
    "        'cterminal_amidation': has_cterm\n",
    "    }\n",
    "\n",
    "def bacterium_to_sample_weight(bacterium, intended_bacterium='E. coli'):\n",
    "    if intended_bacterium in bacterium:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.5\n",
    "    \n",
    "def containing_bacterium(bacterium, df):\n",
    "    return df.loc[df.bacterium.str.contains(bacterium)]\n",
    "\n",
    "def average_over_databases(bacterium_df):\n",
    "    return bacterium_df.groupby('sequence')['value'].mean().dropna()\n",
    "\n",
    "def get_bacterium_df(bacterium, df):\n",
    "    bdf = df.loc[(df.bacterium.str.contains(bacterium))].groupby(['sequence', 'bacterium'])\n",
    "    return bdf.mean().reset_index().dropna()\n",
    "\n",
    "def strip_bad_amino_acids(df, bad_amino_acids=('U', 'X', 'Z')):\n",
    "    for b in bad_amino_acids:\n",
    "        df = df.loc[~df.sequence.str.contains(b)]\n",
    "    return df.reset_index()\n",
    "\n",
    "def split_dataframe(df_to_split,cutoff=0.85):\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    for i in range(len(df_to_split)):\n",
    "        if 'C' not in df_to_split['sequence'][i] and random.random()>cutoff:\n",
    "            test_indices.append(i)\n",
    "        else:\n",
    "            train_indices.append(i)\n",
    "    train_df = df_to_split.iloc[train_indices]\n",
    "    test_df = df_to_split.iloc[test_indices]\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ecoli_train_with_c = pd.read_pickle('Saved_variables/ecoli_train_with_c_df.pkl')\n",
    "ecoli_train_no_c = pd.read_pickle('Saved_variables/ecoli_train_no_c_df.pkl')\n",
    "ecoli_test = pd.read_pickle('Saved_variables/ecoli_test_df.pkl')\n",
    "ecoli_df = pd.read_pickle('Saved_variables/ecoli_all_df.pkl')\n",
    "ecoli_df_no_c = pd.read_pickle('Saved_variables/ecoli_all_no_c_df.pkl')\n",
    "all_df = pd.read_pickle('Saved_variables/all_df.pkl')\n",
    "ecoli_train_train_no_c = pd.read_pickle('Saved_variables/ecoli_train_train_no_c_df.pkl')\n",
    "ecoli_validate_no_c = pd.read_pickle('Saved_variables/ecoli_validate_no_c_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating the train-test split (only do this once)\n",
    "# ecoli_train_train_no_c, ecoli_validate_no_c = train_test_split(ecoli_train_no_c,test_size=0.2)\n",
    "# ecoli_train_train_no_c.to_pickle('Saved_variables/ecoli_train_train_no_c_df.pkl')\n",
    "# ecoli_validate_no_c.to_pickle('Saved_variables/ecoli_validate_no_c_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b62_embedding = generate_embedding(matlist.blosum62,0.997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ecoli_train_input_embed = df_to_input_vec(ecoli_train_no_c,embed_dict=b62_embedding)\n",
    "ecoli_test_one_hot = df_to_input_vec(ecoli_test)\n",
    "ecoli_train_input_one_hot = df_to_input_vec(ecoli_train_no_c)\n",
    "ecoli_train_input_one_hot_ridge = [vec.flatten() for vec in ecoli_train_input_one_hot[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating test and training sets (different featurizations for different ML approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ecoli_train_no_c = ecoli_train_no_c.reset_index(drop=True)\n",
    "ecoli_test = ecoli_test.reset_index(drop=True)\n",
    "train_x_aa_counts = []\n",
    "train_x_seqs = []\n",
    "train_x = []\n",
    "train_y = []\n",
    "train_x_ridge=[]\n",
    "test_x = []\n",
    "test_x_seqs = []\n",
    "test_x_aa_counts = []\n",
    "test_y = []\n",
    "test_x_ridge=[]\n",
    "lengths_1=[]\n",
    "lengths_2=[]\n",
    "for i in range(len(ecoli_train_no_c)):\n",
    "    lengths_1.append(len(ecoli_train_no_c.sequence[i]))\n",
    "    if True:#len(ecoli_train_no_c.sequence[i])<MAX_SEQUENCE_LENGTH:\n",
    "        train_x_seqs.append(ecoli_train_no_c.sequence[i])\n",
    "        train_x_aa_counts.append(sequence_to_aa_counts(ecoli_train_no_c.sequence[i],ecoli_train_no_c.has_cterminal_amidation[i])) \n",
    "        train_y.append(ecoli_train_no_c.value[i])\n",
    "        train_x.append(sequence_to_vector(ecoli_train_no_c.sequence[i],ecoli_train_no_c.has_cterminal_amidation[i]))\n",
    "        train_x_ridge.append(sequence_to_vector(ecoli_train_no_c.sequence[i],ecoli_train_no_c.has_cterminal_amidation[i]).flatten())\n",
    "        lengths_2.append(len(ecoli_train_no_c.sequence[i]))\n",
    "for i in range(len(ecoli_test)):\n",
    "    if len(ecoli_test.sequence[i])<MAX_SEQUENCE_LENGTH:\n",
    "        test_x_seqs.append(ecoli_test.sequence[i])\n",
    "        test_x_aa_counts.append(sequence_to_aa_counts(ecoli_test.sequence[i],ecoli_test.has_cterminal_amidation[i])) \n",
    "        test_y.append(ecoli_test.value[i])\n",
    "        test_x.append(sequence_to_vector(ecoli_test.sequence[i],ecoli_test.has_cterminal_amidation[i]))\n",
    "        test_x_ridge.append(sequence_to_vector(ecoli_test.sequence[i],ecoli_test.has_cterminal_amidation[i]).flatten())\n",
    "        \n",
    "train_x = np.array(train_x)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression on the whole 21x46 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "alphas_to_test = [0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "ridge_train_one_hot = RidgeCV(alphas=alphas_to_test)\n",
    "ridge_train_one_hot.fit(train_x_ridge,train_y)\n",
    "print ridge_train_one_hot.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0\n"
     ]
    }
   ],
   "source": [
    "alphas_to_test = [0.0001,2,4,8,16,24,26,27,28,29,30,31,32,36,40,64]\n",
    "ridge_train_one_hot = RidgeCV(alphas=alphas_to_test)\n",
    "ridge_train_one_hot.fit(train_x_ridge,train_y)\n",
    "print ridge_train_one_hot.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_predicted = ridge_train_one_hot.predict(test_x_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1448ad950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6274677663943323\n",
      "0.43187093180416586\n",
      "MSE: 0.36338475921572205\n"
     ]
    }
   ],
   "source": [
    "plt.plot(ridge_predicted,test_y,'.')\n",
    "plt.show()\n",
    "print np.corrcoef(ridge_predicted,test_y)[0,1]\n",
    "tau,pval = kendalltau(ridge_predicted,test_y)\n",
    "print tau\n",
    "print 'MSE: '+repr(float(np.average([(ridge_predicted[i]-test_y[i])**2 for i in range(len(test_y))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using just the amino acid counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_to_aa_counts(sequence,cterm_amidation):\n",
    "    to_return = []\n",
    "    for char in CHARACTER_LIST:\n",
    "        to_return.append(sequence.count(char))\n",
    "    if cterm_amidation>0.5:\n",
    "        to_return.append(1)\n",
    "    else:\n",
    "        to_return.append(0)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "alphas_to_test = [0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "ridge_train_counts = RidgeCV(alphas=alphas_to_test)\n",
    "ridge_train_counts.fit(train_x_aa_counts,train_y)\n",
    "print ridge_train_counts.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.0\n"
     ]
    }
   ],
   "source": [
    "alphas_to_test = [0.0001,100,120,140,150,160,165,167,170,171,172,173,174,175,180,200,320,640]\n",
    "ridge_train_counts = RidgeCV(alphas=alphas_to_test)\n",
    "ridge_train_counts.fit(train_x_aa_counts,train_y)\n",
    "print ridge_train_counts.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridge_predicted_counts = ridge_train_counts.predict(test_x_aa_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuBNCSAP5/6cXTH/pMtqTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cc15990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ridge_predicted_counts,test_y,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson corr: 0.5601172810273505\n",
      "Kendall tau: 0.3794419789437846\n",
      "MSE: 0.41544297774414196\n"
     ]
    }
   ],
   "source": [
    "print 'Pearson corr: '+repr(np.corrcoef(ridge_predicted_counts,test_y)[0,1])\n",
    "tau,pval = kendalltau(ridge_predicted_counts,test_y)\n",
    "print 'Kendall tau: '+repr(tau)\n",
    "print 'MSE: '+repr(float(np.average([(ridge_predicted_counts[i]-test_y[i])**2 for i in range(len(test_y))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network analysis and comparisons; note default learning rate is 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2895/2895 [==============================] - 9s 3ms/step - loss: 0.6607\n",
      "Epoch 2/100\n",
      "2895/2895 [==============================] - 1s 462us/step - loss: 0.4172\n",
      "Epoch 3/100\n",
      "2895/2895 [==============================] - 1s 494us/step - loss: 0.3710\n",
      "Epoch 4/100\n",
      "2895/2895 [==============================] - 1s 495us/step - loss: 0.3271\n",
      "Epoch 5/100\n",
      "2895/2895 [==============================] - 1s 478us/step - loss: 0.3004\n",
      "Epoch 6/100\n",
      "2895/2895 [==============================] - 1s 491us/step - loss: 0.2835\n",
      "Epoch 7/100\n",
      "2895/2895 [==============================] - 1s 454us/step - loss: 0.2547\n",
      "Epoch 8/100\n",
      "2895/2895 [==============================] - 1s 425us/step - loss: 0.2287\n",
      "Epoch 9/100\n",
      "2895/2895 [==============================] - 1s 376us/step - loss: 0.2244\n",
      "Epoch 10/100\n",
      "2895/2895 [==============================] - 1s 382us/step - loss: 0.2135\n",
      "Epoch 11/100\n",
      "2895/2895 [==============================] - 1s 385us/step - loss: 0.1946\n",
      "Epoch 12/100\n",
      "2895/2895 [==============================] - 1s 391us/step - loss: 0.1852\n",
      "Epoch 13/100\n",
      "2895/2895 [==============================] - 1s 386us/step - loss: 0.1770\n",
      "Epoch 14/100\n",
      "2895/2895 [==============================] - 1s 385us/step - loss: 0.1645\n",
      "Epoch 15/100\n",
      "2895/2895 [==============================] - 1s 391us/step - loss: 0.1629\n",
      "Epoch 16/100\n",
      "2895/2895 [==============================] - 1s 376us/step - loss: 0.1511\n",
      "Epoch 17/100\n",
      "2895/2895 [==============================] - 1s 417us/step - loss: 0.1387\n",
      "Epoch 18/100\n",
      "2895/2895 [==============================] - 1s 445us/step - loss: 0.1367\n",
      "Epoch 19/100\n",
      "2895/2895 [==============================] - 1s 435us/step - loss: 0.1293\n",
      "Epoch 20/100\n",
      "2895/2895 [==============================] - 1s 379us/step - loss: 0.1236\n",
      "Epoch 21/100\n",
      "2895/2895 [==============================] - 1s 397us/step - loss: 0.1192\n",
      "Epoch 22/100\n",
      "2895/2895 [==============================] - 1s 388us/step - loss: 0.1153\n",
      "Epoch 23/100\n",
      "2895/2895 [==============================] - 1s 416us/step - loss: 0.1116\n",
      "Epoch 24/100\n",
      "2895/2895 [==============================] - 2s 576us/step - loss: 0.1114\n",
      "Epoch 25/100\n",
      "2895/2895 [==============================] - 1s 447us/step - loss: 0.1061\n",
      "Epoch 26/100\n",
      "2895/2895 [==============================] - 1s 433us/step - loss: 0.1000\n",
      "Epoch 27/100\n",
      "2895/2895 [==============================] - 1s 458us/step - loss: 0.0978\n",
      "Epoch 28/100\n",
      "2895/2895 [==============================] - 1s 430us/step - loss: 0.0956\n",
      "Epoch 29/100\n",
      "2895/2895 [==============================] - 1s 427us/step - loss: 0.0946\n",
      "Epoch 30/100\n",
      "2895/2895 [==============================] - 1s 512us/step - loss: 0.0896\n",
      "Epoch 31/100\n",
      "2895/2895 [==============================] - 1s 475us/step - loss: 0.0888\n",
      "Epoch 32/100\n",
      "2895/2895 [==============================] - 2s 534us/step - loss: 0.0877\n",
      "Epoch 33/100\n",
      "2895/2895 [==============================] - 2s 569us/step - loss: 0.0857\n",
      "Epoch 34/100\n",
      "2895/2895 [==============================] - 1s 450us/step - loss: 0.0875\n",
      "Epoch 35/100\n",
      "2895/2895 [==============================] - 1s 427us/step - loss: 0.0927\n",
      "Epoch 36/100\n",
      "2895/2895 [==============================] - 1s 454us/step - loss: 0.0875\n",
      "Epoch 37/100\n",
      "2895/2895 [==============================] - 2s 598us/step - loss: 0.0847\n",
      "Epoch 38/100\n",
      "2895/2895 [==============================] - 2s 539us/step - loss: 0.0813\n",
      "Epoch 39/100\n",
      "2895/2895 [==============================] - 1s 452us/step - loss: 0.0829\n",
      "Epoch 40/100\n",
      "2895/2895 [==============================] - 1s 470us/step - loss: 0.0778\n",
      "Epoch 41/100\n",
      "2895/2895 [==============================] - 1s 485us/step - loss: 0.0735\n",
      "Epoch 42/100\n",
      "2895/2895 [==============================] - 1s 467us/step - loss: 0.0760\n",
      "Epoch 43/100\n",
      "2895/2895 [==============================] - 1s 462us/step - loss: 0.0722\n",
      "Epoch 44/100\n",
      "2895/2895 [==============================] - 2s 530us/step - loss: 0.0709\n",
      "Epoch 45/100\n",
      "2895/2895 [==============================] - 1s 443us/step - loss: 0.0710\n",
      "Epoch 46/100\n",
      "2895/2895 [==============================] - 1s 422us/step - loss: 0.0689\n",
      "Epoch 47/100\n",
      "2895/2895 [==============================] - 1s 406us/step - loss: 0.0690\n",
      "Epoch 48/100\n",
      "2895/2895 [==============================] - 1s 413us/step - loss: 0.0710\n",
      "Epoch 49/100\n",
      "2895/2895 [==============================] - 1s 444us/step - loss: 0.0713\n",
      "Epoch 50/100\n",
      "2895/2895 [==============================] - 1s 436us/step - loss: 0.0681\n",
      "Epoch 51/100\n",
      "2895/2895 [==============================] - 1s 499us/step - loss: 0.0649\n",
      "Epoch 52/100\n",
      "2895/2895 [==============================] - 1s 436us/step - loss: 0.0645\n",
      "Epoch 53/100\n",
      "2895/2895 [==============================] - 1s 418us/step - loss: 0.0649\n",
      "Epoch 54/100\n",
      "2895/2895 [==============================] - 1s 417us/step - loss: 0.0621\n",
      "Epoch 55/100\n",
      "2895/2895 [==============================] - 1s 472us/step - loss: 0.0585\n",
      "Epoch 56/100\n",
      "2895/2895 [==============================] - 1s 432us/step - loss: 0.0620\n",
      "Epoch 57/100\n",
      "2895/2895 [==============================] - 1s 438us/step - loss: 0.0578\n",
      "Epoch 58/100\n",
      "2895/2895 [==============================] - 1s 449us/step - loss: 0.0593\n",
      "Epoch 59/100\n",
      "2895/2895 [==============================] - 1s 445us/step - loss: 0.0566\n",
      "Epoch 60/100\n",
      "2895/2895 [==============================] - 1s 444us/step - loss: 0.0591\n",
      "Epoch 61/100\n",
      "2895/2895 [==============================] - 1s 442us/step - loss: 0.0565\n",
      "Epoch 62/100\n",
      "2895/2895 [==============================] - 1s 480us/step - loss: 0.0552\n",
      "Epoch 63/100\n",
      "2895/2895 [==============================] - 1s 415us/step - loss: 0.0579\n",
      "Epoch 64/100\n",
      "2895/2895 [==============================] - 1s 437us/step - loss: 0.0553\n",
      "Epoch 65/100\n",
      "2895/2895 [==============================] - 1s 442us/step - loss: 0.0553\n",
      "Epoch 66/100\n",
      "2895/2895 [==============================] - 1s 425us/step - loss: 0.0562\n",
      "Epoch 67/100\n",
      "2895/2895 [==============================] - 1s 430us/step - loss: 0.0548\n",
      "Epoch 68/100\n",
      "2895/2895 [==============================] - 1s 401us/step - loss: 0.0611\n",
      "Epoch 69/100\n",
      "2895/2895 [==============================] - 1s 421us/step - loss: 0.0545\n",
      "Epoch 70/100\n",
      "2895/2895 [==============================] - 1s 435us/step - loss: 0.0532\n",
      "Epoch 71/100\n",
      "2895/2895 [==============================] - 1s 430us/step - loss: 0.0562\n",
      "Epoch 72/100\n",
      "2895/2895 [==============================] - 1s 409us/step - loss: 0.0535\n",
      "Epoch 73/100\n",
      "2895/2895 [==============================] - 1s 393us/step - loss: 0.0556\n",
      "Epoch 74/100\n",
      "2895/2895 [==============================] - 1s 417us/step - loss: 0.0524\n",
      "Epoch 75/100\n",
      "2895/2895 [==============================] - 1s 424us/step - loss: 0.0506\n",
      "Epoch 76/100\n",
      "2895/2895 [==============================] - 1s 394us/step - loss: 0.0513\n",
      "Epoch 77/100\n",
      "2895/2895 [==============================] - 1s 390us/step - loss: 0.0483\n",
      "Epoch 78/100\n",
      "2895/2895 [==============================] - 1s 381us/step - loss: 0.0474\n",
      "Epoch 79/100\n",
      "2895/2895 [==============================] - 1s 378us/step - loss: 0.0488\n",
      "Epoch 80/100\n",
      "2895/2895 [==============================] - 1s 415us/step - loss: 0.0475\n",
      "Epoch 81/100\n",
      "2895/2895 [==============================] - 1s 465us/step - loss: 0.0498\n",
      "Epoch 82/100\n",
      "2895/2895 [==============================] - 2s 526us/step - loss: 0.0464\n",
      "Epoch 83/100\n",
      "2895/2895 [==============================] - 2s 560us/step - loss: 0.0475\n",
      "Epoch 84/100\n",
      "2895/2895 [==============================] - 1s 476us/step - loss: 0.0471\n",
      "Epoch 85/100\n",
      "2895/2895 [==============================] - 1s 508us/step - loss: 0.0500\n",
      "Epoch 86/100\n",
      "2895/2895 [==============================] - 1s 458us/step - loss: 0.0447\n",
      "Epoch 87/100\n",
      "2895/2895 [==============================] - 1s 482us/step - loss: 0.0476\n",
      "Epoch 88/100\n",
      "2895/2895 [==============================] - 1s 493us/step - loss: 0.0460\n",
      "Epoch 89/100\n",
      "2895/2895 [==============================] - 1s 454us/step - loss: 0.0449\n",
      "Epoch 90/100\n",
      "2895/2895 [==============================] - 1s 489us/step - loss: 0.0452\n",
      "Epoch 91/100\n",
      "2895/2895 [==============================] - 1s 493us/step - loss: 0.0418\n",
      "Epoch 92/100\n",
      "2895/2895 [==============================] - 1s 428us/step - loss: 0.0427\n",
      "Epoch 93/100\n",
      "2895/2895 [==============================] - 1s 481us/step - loss: 0.0482\n",
      "Epoch 94/100\n",
      "2895/2895 [==============================] - 1s 496us/step - loss: 0.0428\n",
      "Epoch 95/100\n",
      "2895/2895 [==============================] - 1s 470us/step - loss: 0.0412\n",
      "Epoch 96/100\n",
      "2895/2895 [==============================] - 1s 431us/step - loss: 0.0424\n",
      "Epoch 97/100\n",
      "2895/2895 [==============================] - 1s 415us/step - loss: 0.0430\n",
      "Epoch 98/100\n",
      "2895/2895 [==============================] - 1s 424us/step - loss: 0.0444\n",
      "Epoch 99/100\n",
      "2895/2895 [==============================] - 1s 496us/step - loss: 0.0439\n",
      "Epoch 100/100\n",
      "2895/2895 [==============================] - 1s 494us/step - loss: 0.0410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11cd61e10>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model_normal = conv_model()\n",
    "conv_model_normal.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_kernel4 = conv_model(kernelsize=4)\n",
    "# conv_model_kernel4.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_kernel6 = conv_model(kernelsize=6)\n",
    "# conv_model_kernel6.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_kernel3 = conv_model(kernelsize=3)\n",
    "# conv_model_kernel3.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_kernel2 = conv_model(kernelsize=2)\n",
    "# conv_model_kernel2.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_kernel7 = conv_model(kernelsize=7)\n",
    "# conv_model_kernel7.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_kernel1 = conv_model(kernelsize=1)\n",
    "# conv_model_kernel1.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_one_layer = one_layer_conv_model()\n",
    "# conv_model_one_layer.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_moved_dropout = conv_model_move_dropout()\n",
    "# conv_model_moved_dropout.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_p9_dropout = conv_model_set_dropout(dropout_level=0.9)\n",
    "# conv_model_p9_dropout.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_p7_dropout = conv_model_set_dropout(dropout_level=0.7)\n",
    "# conv_model_p7_dropout.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# conv_model_p3_dropout = conv_model_set_dropout(dropout_level=0.3)\n",
    "# conv_model_p3_dropout.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# rnn_one_layer = one_layer_recurrent_model()\n",
    "# rnn_one_layer.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# rnn_two_layers = two_layer_recurrent_model()\n",
    "# rnn_two_layers.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# rnn_three_layers = three_layer_recurrent_model()\n",
    "# rnn_three_layers.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# lstm_mod = lstm_model()\n",
    "# lstm_mod.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# bi_rnn = birecurrent_model()\n",
    "# bi_rnn.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# bi_lstm = bilstm_model()\n",
    "# bi_lstm.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# bi_rnn_p01 = birecurrent_model(learn_rate=0.01)\n",
    "# bi_rnn_p01.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# bi_rnn_p1 = birecurrent_model(learn_rate=0.1)\n",
    "# bi_rnn_p1.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# bi_rnn_p0001 = birecurrent_model(learn_rate=0.0001)\n",
    "# bi_rnn_p0001.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# bi_rnn_p00001 = birecurrent_model(learn_rate=0.00001)\n",
    "# bi_rnn_p00001.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# lstm_p01 = lstm_model(learn_rate=0.01)\n",
    "# lstm_p01.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# lstm_p1 = lstm_model(learn_rate=0.1)\n",
    "# lstm_p1.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# lstm_p05 = lstm_model(learn_rate=0.05)\n",
    "# lstm_p05.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# lstm_p02 = lstm_model(learn_rate=0.02)\n",
    "# lstm_p02.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# lstm_p0001 = lstm_model(learn_rate=0.0001)\n",
    "# lstm_p0001.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# lstm_p00001 = lstm_model(learn_rate=0.00001)\n",
    "# lstm_p00001.fit(train_x,train_y,batch_size=40,epochs=100)\n",
    "\n",
    "# bi_lstm_300 = bilstm_model()\n",
    "# bi_lstm_300.fit(train_x,train_y,batch_size=40,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    predicted = model.predict(test_x)\n",
    "    predicted = [pred[0] for pred in predicted]\n",
    "#     plt.plot(predicted,test_y,'.')\n",
    "#     plt.show()\n",
    "    to_return = 'RMSE,'+repr(np.sqrt(float(np.average([(predicted[i]-test_y[i])**2 for i in range(len(test_y))]))))\n",
    "    to_return += ',Pearson Correlation,'+repr(float(np.corrcoef(predicted,test_y)[1,0]))\n",
    "    tau,pval = kendalltau(predicted,test_y)\n",
    "    to_return += ',Kendall tau Correlation,'+repr(tau)\n",
    "    to_return += ',MSE,'+repr(float(np.average([(predicted[i]-test_y[i])**2 for i in range(len(test_y))])))\n",
    "    print to_return\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE,0.5017459404365912,Pearson Correlation,0.7700296338278537,Kendall tau Correlation,0.5705882658527176,MSE,0.2517489887445993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RMSE,0.5017459404365912,Pearson Correlation,0.7700296338278537,Kendall tau Correlation,0.5705882658527176,MSE,0.2517489887445993'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(conv_model_normal)\n",
    "# evaluate_model(conv_model_kernel1)\n",
    "# evaluate_model(conv_model_kernel2)\n",
    "# evaluate_model(conv_model_kernel3)\n",
    "# evaluate_model(conv_model_kernel4)\n",
    "# evaluate_model(conv_model_kernel6)\n",
    "# evaluate_model(conv_model_kernel7)\n",
    "# evaluate_model(conv_model_one_layer)\n",
    "# evaluate_model(conv_model_moved_dropout)\n",
    "# evaluate_model(conv_model_p9_dropout)\n",
    "# evaluate_model(conv_model_p7_dropout)\n",
    "# evaluate_model(conv_model_p3_dropout)\n",
    "# evaluate_model(rnn_one_layer)\n",
    "# evaluate_model(rnn_two_layers)\n",
    "# evaluate_model(rnn_three_layers)\n",
    "# evaluate_model(bi_rnn)\n",
    "# evaluate_model(lstm_p00001)\n",
    "# evaluate_model(lstm_p0001)\n",
    "# evaluate_model(lstm_mod)\n",
    "# evaluate_model(lstm_p01)\n",
    "# evaluate_model(lstm_p1)\n",
    "# evaluate_model(bi_lstm)\n",
    "# evaluate_model(lstm_p02)\n",
    "# evaluate_model(lstm_p05)\n",
    "# evaluate_model(bi_rnn_p01)\n",
    "# evaluate_model(bi_rnn_p1)\n",
    "# evaluate_model(bi_rnn_p0001)\n",
    "# evaluate_model(bi_rnn_p00001)\n",
    "# evaluate_model(bi_lstm_300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "class kNN_model:\n",
    "    def __init__(self,seq_db,mic_vals,k,local=True,score_matrix=matlist.ident,gap_open_penalty=-9,gap_extension_penalty=-1,edit_distance=False):\n",
    "        self.seq_db = seq_db\n",
    "        self.k = k\n",
    "        self.mic_vals = mic_vals\n",
    "        self.local = local\n",
    "        self.score_matrix = score_matrix\n",
    "        self.gap_open_penalty = gap_open_penalty\n",
    "        self.gap_extension_penalty = gap_extension_penalty\n",
    "        self.edit_distance = edit_distance\n",
    "    def predict_knn(self,seq):\n",
    "        alignment_scores = [0]*len(self.seq_db)\n",
    "        for i in range(len(self.seq_db)):\n",
    "            sequence = self.seq_db[i]\n",
    "            if self.edit_distance:\n",
    "                alignment_scores[i] = -1*levenshtein(seq,sequence)\n",
    "            elif self.local:\n",
    "                alignment_scores[i] = pairwise2.align.localds(seq,sequence,self.score_matrix,self.gap_open_penalty,self.gap_extension_penalty,score_only=True)\n",
    "            else:\n",
    "                alignment_scores[i] = pairwise2.align.globalds(seq,sequence,self.score_matrix,self.gap_open_penalty,self.gap_extension_penalty,score_only=True)\n",
    "        indices = np.argpartition(alignment_scores,-1*self.k)[(-1*self.k):]\n",
    "        guess=0\n",
    "        for index in indices:\n",
    "            guess += float(self.mic_vals[index])\n",
    "        return guess/self.k\n",
    "    def prediction_with_similarity(self,seq):\n",
    "        alignment_scores = [0]*len(self.seq_db)\n",
    "        for i in range(len(self.seq_db)):\n",
    "            sequence = self.seq_db[i]\n",
    "            if self.edit_distance:\n",
    "                alignment_scores[i] = -1*levenshtein(seq,sequence)\n",
    "            elif self.local:\n",
    "                alignment_scores[i] = pairwise2.align.localds(seq,sequence,self.score_matrix,self.gap_open_penalty,self.gap_extension_penalty,score_only=True)\n",
    "            else:\n",
    "                alignment_scores[i] = pairwise2.align.globalds(seq,sequence,self.score_matrix,self.gap_open_penalty,self.gap_extension_penalty,score_only=True)\n",
    "        indices = np.argpartition(alignment_scores,-1*self.k)[(-1*self.k):]\n",
    "        max_align_score = max(alignment_scores)\n",
    "        guess=0\n",
    "        for index in indices:\n",
    "            guess += float(self.mic_vals[index])\n",
    "        return max_align_score,guess/self.k\n",
    "    def error_vs_align_score(self,seqs,test_mics):\n",
    "        scores = []\n",
    "        sqrt_errors = []\n",
    "        for i,seq in enumerate(seqs):\n",
    "            align_score,prediction = self.prediction_with_similarity(seq)\n",
    "            scores.append(align_score)\n",
    "            sqrt_errors.append(np.sqrt((prediction-test_mics[i])**2))\n",
    "        return scores,sqrt_errors     \n",
    "    def predict(self,seqs):\n",
    "        return [self.predict_knn(seq) for seq in seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_knn_model(knn_model):\n",
    "    predicted = knn_model.predict(test_x_seqs)\n",
    "#     predicted = [pred[0] for pred in predicted]\n",
    "#     plt.plot(predicted,test_y,'.')\n",
    "#     plt.show()\n",
    "#     print 'MSE: '+repr(float(np.average([(predicted[i]-test_y[i])**2 for i in range(len(test_y))])))\n",
    "#     print 'Pearson Correlation: '+repr(float(np.corrcoef(predicted,test_y)[1,0]))\n",
    "    tau,pval = kendalltau(predicted,test_y)\n",
    "#     print 'Kendall tau Correlation: '+repr(tau)\n",
    "    mse = 'MSE,'+repr(float(np.average([(predicted[i]-test_y[i])**2 for i in range(len(test_y))])))\n",
    "    pearson = 'Pearson,'+repr(float(np.corrcoef(predicted,test_y)[1,0]))\n",
    "    tau = 'Kendall tau,'+repr(tau)\n",
    "    return mse+','+pearson+','+tau+','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k,5,Edit distance,n/a,MSE,0.2957386359801885,Pearson,0.7160638862911988,Kendall tau,0.5042597322796677,\n"
     ]
    }
   ],
   "source": [
    "# Using the best model based on validation set\n",
    "knn_model = kNN_model(train_x_seqs,train_y,5,edit_distance=True)\n",
    "print 'k,5,Edit distance,n/a,'+evaluate_knn_model(knn_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k,7,Edit distance,n/a,MSE,0.3274507158382785,Pearson,0.6809477603880043,Kendall tau,0.46321277683261175,\n",
      "k,7,Local,Pam30,MSE,0.4526885352717089,Pearson,0.5633828028223556,Kendall tau,0.36606249247518946,\n",
      "k,7,Global,Pam30,MSE,0.3806242975015145,Pearson,0.6264822905994067,Kendall tau,0.40547849088342036,\n",
      "k,7,Local,Identity,MSE,0.4383547597628035,Pearson,0.5827605124519708,Kendall tau,0.37579977635356787,\n",
      "k,7,Global,Identity,MSE,0.36692862058403447,Pearson,0.6423241268828116,Kendall tau,0.42125075354140473,\n",
      "k,8,Edit distance,n/a,MSE,0.3340565313305525,Pearson,0.6731233730264672,Kendall tau,0.459712301164396,\n",
      "k,8,Local,Pam30,MSE,0.4455778075656142,Pearson,0.5712035153263394,Kendall tau,0.36638180524951564,\n",
      "k,8,Global,Pam30,MSE,0.38298487393425007,Pearson,0.6225764746295971,Kendall tau,0.4040353159282329,\n",
      "k,8,Local,Identity,MSE,0.44224383420287583,Pearson,0.5750572594388242,Kendall tau,0.366543694729149,\n",
      "k,8,Global,Identity,MSE,0.3692584254390431,Pearson,0.6388163958967668,Kendall tau,0.4170535479147295,\n"
     ]
    }
   ],
   "source": [
    "for k in range(7,9):\n",
    "    knn_model = kNN_model(train_x_seqs,train_y,k,edit_distance=True)\n",
    "    print 'k,'+repr(k)+',Edit distance,n/a,'+evaluate_knn_model(knn_model)\n",
    "    knn_model = kNN_model(train_x_seqs,train_y,k,score_matrix=matlist.pam30)\n",
    "    print 'k,'+repr(k)+',Local,Pam30,'+evaluate_knn_model(knn_model)\n",
    "    knn_model = kNN_model(train_x_seqs,train_y,k,local=False,score_matrix=matlist.pam30)\n",
    "    print 'k,'+repr(k)+',Global,Pam30,'+evaluate_knn_model(knn_model)\n",
    "    knn_model = kNN_model(train_x_seqs,train_y,k)\n",
    "    print 'k,'+repr(k)+',Local,Identity,'+evaluate_knn_model(knn_model)\n",
    "    knn_model = kNN_model(train_x_seqs,train_y,k,local=False)\n",
    "    print 'k,'+repr(k)+',Global,Identity,'+evaluate_knn_model(knn_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
